{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29630b77-0a81-40f4-a561-39faf8a0a389",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7538f6a-0947-444b-ba0a-90b3261fdfcf",
   "metadata": {},
   "source": [
    "## Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68952f8-212b-4bb2-8a1d-444bbabc9180",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Нужно обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Необходимо построить модель со значением метрики качества F1 не меньше 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af9b78-fbd5-4485-81cd-46a1670f0401",
   "metadata": {},
   "source": [
    "## План работ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3b1e4-7631-4684-963d-282d6fe9dc3e",
   "metadata": {},
   "source": [
    "1. Загрузить и подготовить данные.\n",
    "2. Обучить разные модели.\n",
    "3. Сделать выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0af82-ad65-44d1-beaf-bd9d98e0465e",
   "metadata": {},
   "source": [
    "## Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e625cf-aa8f-4032-923a-6f0d847a96f3",
   "metadata": {},
   "source": [
    "Данные находятся в файле *toxic_comments.csv*. \n",
    "Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c829e-e094-4358-a866-a58bae2875b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e848bda-b511-4081-a1b1-d767fa19913c",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479176a7-44c4-4405-a881-53c3bbba8105",
   "metadata": {},
   "source": [
    "Сперва установим и импортируем все необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b622b62f-b208-43ca-b7d7-45f619e879b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install protobuf==3.20.*\n",
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2576454-aa86-4383-8a9b-f428e1b87943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from numpy.random import RandomState\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, train_test_split\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a22cf-b6c8-4303-965d-090a9bd80799",
   "metadata": {},
   "source": [
    "И произведем некоторые настройки параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3fd049-e98e-4c04-b098-cde27cb91a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pashn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pashn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "torch.cuda.empty_cache()\n",
    "state = RandomState(12345)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56588843-3fee-4980-8d5e-78c3da2fc61e",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedfb581-733b-4cb5-9937-8136a4a4e32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"toxic_comments.csv\")\n",
    "\n",
    "except:\n",
    "    df = pd.read_csv(\"/datasets/toxic_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783e93ef-c3eb-4b50-8ec3-8e1f3705e703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25184</th>\n",
       "      <td>25208</td>\n",
       "      <td>sky brightness graph \\n\\nAlbester, where did u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143653</th>\n",
       "      <td>143807</td>\n",
       "      <td>Over-use of shy template \\n\\nI have reversed y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142924</th>\n",
       "      <td>143078</td>\n",
       "      <td>\"\\n\\nThere is far too much pointless listing o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151285</th>\n",
       "      <td>151441</td>\n",
       "      <td>Good point, thanks. I think I copied the R not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38929</th>\n",
       "      <td>38978</td>\n",
       "      <td>\"\\n\\nMerge of Emerald Ash borer \"\"Infestation\"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic\n",
       "25184        25208  sky brightness graph \\n\\nAlbester, where did u...      0\n",
       "143653      143807  Over-use of shy template \\n\\nI have reversed y...      0\n",
       "142924      143078  \"\\n\\nThere is far too much pointless listing o...      0\n",
       "151285      151441  Good point, thanks. I think I copied the R not...      0\n",
       "38929        38978  \"\\n\\nMerge of Emerald Ash borer \"\"Infestation\"...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2637d-d5ad-40e2-b22c-dfb699cc212f",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed45bba-7cd9-46d9-a36a-90413c6fbfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общая информация о датафрейме:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Общая информация о датафрейме:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da89e41-1497-4615-bc99-691a1ef447d6",
   "metadata": {},
   "source": [
    "Как видим, пропусков нет. Но есть колонка *Unnamed: 0* про которую не известно, за что она отвечает, да и по значениям, кажется что она не сильно информативна (просто в какие то моменты начинает немного отличаться от значений индекса). Избавимся от этой колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f10f1a6-4e73-438f-a8c3-7a2f25fa0f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e40e2-f6ca-41d9-9240-b1e4aa3dc2c3",
   "metadata": {},
   "source": [
    "Далее проверим  данные на наличие дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c45de98-7b0e-48ed-9590-125f7403e4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов =  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Количество дубликатов = \", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d416945-3e08-4c48-a0ab-6cd070e5268d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Дубликатов нет, отлично!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc60e5-1e0d-4ab7-a276-2f5f5def82c3",
   "metadata": {},
   "source": [
    "Теперь проведем лемматизацию текстов и избавимся от лишних символов с помощью регулярных выражений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea3f5ef-9777-4bfd-960e-d2c6e794d8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = \" \".join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n",
    "\n",
    "\n",
    "def clear_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \" \", text)\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e7ec22-c83b-4492-849a-fb2fea5fe6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:01<00:00, 103037.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст после регулярных выражений:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation Why the edits made under my userna...      0\n",
       "1  D aww He matches this background colour I m se...      0\n",
       "2  Hey man I m really not trying to edit war It s...      0\n",
       "3  More I can t make any real suggestions on impr...      0\n",
       "4  You sir are my hero Any chance you remember wh...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:59<00:00, 2671.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст после лемматизации:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation Why the edits made under my userna...      0\n",
       "1  D aww He match this background colour I m seem...      0\n",
       "2  Hey man I m really not trying to edit war It s...      0\n",
       "3  More I can t make any real suggestion on impro...      0\n",
       "4  You sir are my hero Any chance you remember wh...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст после перевода в нижний регистр:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  explanation why the edits made under my userna...      0\n",
       "1  d aww he match this background colour i m seem...      0\n",
       "2  hey man i m really not trying to edit war it s...      0\n",
       "3  more i can t make any real suggestion on impro...      0\n",
       "4  you sir are my hero any chance you remember wh...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].progress_apply(clear_text)\n",
    "print(\"Текст после регулярных выражений:\")\n",
    "display(df.head())\n",
    "df[\"text\"] = df[\"text\"].progress_apply(lemmatize)\n",
    "print(\"Текст после лемматизации:\")\n",
    "display(df.head())\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "print(\"Текст после перевода в нижний регистр:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab77bd-ee25-44a1-87af-4071ade04506",
   "metadata": {},
   "source": [
    "### Промежуточный вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4317ed6-293d-425a-9690-46ae0c878cd7",
   "metadata": {},
   "source": [
    "1. Подключили необходимые библиотеки\n",
    "2. Загрузили данные\n",
    "3. Определили что нет пропусков и дубликатов\n",
    "4. Очистили текст с помощью регулярных выражений\n",
    "5. Лемматизировали данные\n",
    "6. Избавились от неинформативной колонки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39972c3-eedf-45dd-bde8-7f81ea61b3c1",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3ef3d-f1e9-42cf-820b-c8586fbf5828",
   "metadata": {},
   "source": [
    "Попробуем два подхода:\n",
    "* эмбеддинг с помощью готовой модели ***Bert***\n",
    "* векторизация с помощью метрики ***TF-IDF***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf6c47-db64-4786-acd3-6a12447d4166",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11232d-e157-4f60-a8fd-20c462e53aaf",
   "metadata": {},
   "source": [
    "#### Эмбеддинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5aaed-19fd-4b4b-9fa7-2094c3c4b96c",
   "metadata": {},
   "source": [
    "Перед тем как начать обучать модели необходимо создать матрицу признаков из эмбеддингов. Для этого воспользуемся моделью ***BERT***. \n",
    "\n",
    "Сперва токенизируем наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4322806-cb96-4ce9-a45a-14d0af2bb639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [02:41<00:00, 988.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing...\")\n",
    "tokenizer = transformers.BertTokenizer(vocab_file=\"vocab.txt\")\n",
    "tokenized = df[\"text\"].progress_apply(\n",
    "    lambda x: tokenizer.encode(\n",
    "        x, truncation=True, add_special_tokens=True, max_length=512\n",
    "    )\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ec40c-ec63-40b5-a1ab-09e6556fb4a3",
   "metadata": {},
   "source": [
    "Поиск максимального количества токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa7e9da-eb04-4bfa-aef3-9dab02d01923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max length: 100%|██████████| 159292/159292 [00:00<00:00, 3630391.35it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tqdm(tokenized.values, desc=\"Max length\"):\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b27e29-2499-487e-9450-ce1caf358ba3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Padding, чтобы длины данных в корпусе были равными, и создание маски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb5628b-5a51-40bd-ba3b-a49e0afd1163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding: 100%|██████████| 159292/159292 [00:01<00:00, 117834.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159292, 512)\n"
     ]
    }
   ],
   "source": [
    "padded = np.array(\n",
    "    [i + [0] * (max_len - len(i)) for i in tqdm(tokenized.values, desc=\"Padding\")]\n",
    ")\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0cadc-d86a-47b9-aa48-ed8cc8ce2f6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Инициализируем саму модель класса BertModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974ff23a-de58-45e7-9527-09b05fc3ce79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# код на случай скачанных моделей Bert\n",
    "# config = transformers.BertConfig.from_json_file(\"bert_config.json\")\n",
    "# model = transformers.BertForPreTraining.from_pretrained(\n",
    "#    \"bert_model.ckpt.index\", from_tf=True, config=config\n",
    "# )\n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = model.to(\"cuda\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f6389-68e3-4dca-8708-3f6209b7a8f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Преобразуем данные в эмбеддинги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "becf8fbb-1535-4f4b-b619-d1ced5db6c41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79646/79646 [1:22:19<00:00, 16.12it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "embeddings = []\n",
    "for i in tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.cuda.LongTensor(padded[batch_size * i : batch_size * (i + 1)])\n",
    "    attention_mask_batch = torch.cuda.LongTensor(\n",
    "        attention_mask[batch_size * i : batch_size * (i + 1)]\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append((batch_embeddings[0][:, 0, :]).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "044ee0bb-e8b0-46df-a7fe-bd53b9b6feea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119469, 768)\n",
      "(39823, 768)\n"
     ]
    }
   ],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, df[\"toxic\"], test_size=0.25, random_state=state\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b9e0a-65b3-4a35-b0a3-ac23cb3cf6c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f45aefda-c218-42a5-b8f8-232d9448c866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "8 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1252, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1179, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the train scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [ nan  nan  nan  nan 0.08 0.08]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the train scores are non-finite: [nan nan nan nan  1.  1.]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика f1 на тестовой выборке =  0.63\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"max_iter\": [100, 500, 700, 1000],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "}\n",
    "\n",
    "linear_regression = HalvingRandomSearchCV(\n",
    "    estimator=LogisticRegression(random_state=state, class_weight=\"balanced\"),\n",
    "    param_distributions=param_distributions,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=state,\n",
    "    scoring=\"f1\",\n",
    "    max_resources=80,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Метрика f1 на тестовой выборке = \",\n",
    "    round(\n",
    "        f1_score(y_test, linear_regression.best_estimator_.predict(X_test)),\n",
    "        2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255d9b9-5d46-4b26-bc67-7761204cae53",
   "metadata": {},
   "source": [
    "#### Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80133f47-bded-42d6-98ed-b828491cec9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика f1 на тестовой выборке =  0.24\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"max_depth\": [1, 5, 10, 50, 100],\n",
    "    \"n_estimators\": [2, 10, 50, 100, 300],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "}\n",
    "\n",
    "\n",
    "rf = HalvingRandomSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=state),\n",
    "    param_distributions=param_distributions,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=state,\n",
    "    scoring=\"f1\",\n",
    "    max_resources=1500,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Метрика f1 на тестовой выборке = \",\n",
    "    round(f1_score(y_test, rf.best_estimator_.predict(X_test)), 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8a976-747e-40c8-8af9-159603806aca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0a9b90-f751-48e4-ad95-7dc961a30e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12142, number of negative: 107327\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 119469, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101633 -> initscore=-2.179210\n",
      "[LightGBM] [Info] Start training from score -2.179210\n",
      "Метрика f1 на тестовой выборке =  0.66\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"n_estimators\": [10, 50, 70, 100, 150, 200, 500],\n",
    "    \"num_leaves\": [2, 5, 10, 30, 50, 100, 150],\n",
    "    \"learning_rate\": [0.03, 0.1, 0.2, 0.5, 0.7],\n",
    "}\n",
    "\n",
    "\n",
    "LightGBM = HalvingRandomSearchCV(\n",
    "    estimator=lgbm.LGBMClassifier(random_state=state, force_col_wise=True),\n",
    "    param_distributions=param_distributions,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=state,\n",
    "    scoring=\"f1\",\n",
    "    max_resources=2450,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Метрика f1 на тестовой выборке = \",\n",
    "    round(f1_score(y_test, LightGBM.best_estimator_.predict(X_test)), 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190daa6-dae1-4ed0-a7ee-0238b49752ce",
   "metadata": {},
   "source": [
    "#### Промежуточный вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c7c62-af60-4b7c-8e29-b4874b6112e4",
   "metadata": {},
   "source": [
    "1. С помощью модели **BERT** провели эмбеддинг признаков\n",
    "2. Обучили три модели и получили следующие значения метрик *f1*:\n",
    "    * Логистическая регрессия - 0.63\n",
    "    * Случайный лес - 0.24\n",
    "    * LightGBM - 0.66\n",
    "    \n",
    "Лучший результат показала модель *LightGBM*. Но тем не менее ни одна из моделей не смогла показать результат выше целевого значения 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cc7a9-67dc-42a9-b29f-c4674dd459c1",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c1014-9954-4494-85ab-da9509be81e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41806805-bbda-4fd3-8ea9-ffa796f87f8b",
   "metadata": {},
   "source": [
    "Данные уже лемматизированы и очищены от ненужных символов. Произведем векторизацию текста, а перед этим разделим данные на обучающую и тестовую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a8cbab2-29a9-415b-8365-737426e6c053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119469,)\n",
      "(39823,)\n"
     ]
    }
   ],
   "source": [
    "X_train_tf_idf, X_test_tf_idf, y_train_tf_idf, y_test_tf_idf = train_test_split(\n",
    "    df[\"text\"], df[\"toxic\"], test_size=0.25, random_state=state\n",
    ")\n",
    "\n",
    "stopwords = set(nltk_stopwords.words(\"english\"))\n",
    "count_tf_idf_train = TfidfVectorizer(stop_words=list(stopwords))\n",
    "\n",
    "tf_idf_train = count_tf_idf_train.fit_transform(X_train_tf_idf)\n",
    "tf_idf_test = count_tf_idf_train.transform(X_test_tf_idf)\n",
    "\n",
    "print(X_train_tf_idf.shape)\n",
    "print(X_test_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a78c22-dcba-43bf-9994-5098534c4894",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5268fe5f-7922-4665-9403-92d795c25861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "4 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1252, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1179, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the train scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan  0.  0.]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\ds_practicum_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the train scores are non-finite: [nan nan nan nan  1.  1.]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика f1 на тестовой выборке =  0.75\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"max_iter\": [100, 500, 700, 1000],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "}\n",
    "\n",
    "linear_regression = HalvingRandomSearchCV(\n",
    "    estimator=LogisticRegression(random_state=state, class_weight=\"balanced\"),\n",
    "    param_distributions=param_distributions,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=state,\n",
    "    scoring=\"f1\",\n",
    "    max_resources=80,\n",
    ").fit(tf_idf_train, y_train_tf_idf)\n",
    "\n",
    "print(\n",
    "    \"Метрика f1 на тестовой выборке = \",\n",
    "    round(\n",
    "        f1_score(y_test_tf_idf, linear_regression.best_estimator_.predict(tf_idf_test)),\n",
    "        2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a9ed6-f2de-4d94-8380-90e2417e9350",
   "metadata": {},
   "source": [
    "#### Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f71f4fc-717b-4f65-84da-7cac98b67ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика f1 на тестовой выборке =  0.0\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"max_depth\": [1, 5, 10, 50, 100],\n",
    "    \"n_estimators\": [2, 10, 50, 100, 300],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "}\n",
    "\n",
    "\n",
    "rf = HalvingRandomSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=state),\n",
    "    param_distributions=param_distributions,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=state,\n",
    "    scoring=\"f1\",\n",
    "    max_resources=1500,\n",
    ").fit(tf_idf_train, y_train_tf_idf)\n",
    "\n",
    "print(\n",
    "    \"Метрика f1 на тестовой выборке = \",\n",
    "    round(f1_score(y_test_tf_idf, rf.best_estimator_.predict(tf_idf_test)), 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6d9d8-9759-4815-91c7-84837fee6a7b",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a5e107-fdec-4198-abab-05995c8f6830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12208, number of negative: 107261\n",
      "[LightGBM] [Info] Total Bins 576654\n",
      "[LightGBM] [Info] Number of data points in the train set: 119469, number of used features: 10945\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102186 -> initscore=-2.173174\n",
      "[LightGBM] [Info] Start training from score -2.173174\n",
      "Метрика f1 на тестовой выборке =  0.71\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"n_estimators\": [10, 100, 200, 500],\n",
    "    \"num_leaves\": [2, 5, 10, 100, 150],\n",
    "    \"learning_rate\": [0.03, 0.1, 0.7],\n",
    "}\n",
    "\n",
    "\n",
    "LightGBM = HalvingRandomSearchCV(\n",
    "    estimator=lgbm.LGBMClassifier(random_state=state, force_col_wise=True),\n",
    "    param_distributions=param_distributions,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=state,\n",
    "    scoring=\"f1\",\n",
    "    max_resources=600,\n",
    ").fit(tf_idf_train, y_train_tf_idf)\n",
    "\n",
    "print(\n",
    "    \"Метрика f1 на тестовой выборке = \",\n",
    "    round(f1_score(y_test_tf_idf, LightGBM.best_estimator_.predict(tf_idf_test)), 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de2f2a-1abb-4e47-88c7-5b1bd297a2e0",
   "metadata": {},
   "source": [
    "#### Промежуточный итог"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f6867-9073-42e2-8ad4-2e47d6232e6f",
   "metadata": {},
   "source": [
    "1. С помощью метрики **TF-IDF** векторизовали признаки\n",
    "2. Обучили три модели и получили следующие значения метрик *f1*:\n",
    "    * Логистическая регрессия - 0.75\n",
    "    * Случайный лес - 0.0 (не понимаю почему)\n",
    "    * LightGBM - 0.71\n",
    "    \n",
    "Лучший результат показала модель *Логистическая регрессия*, достигнув целевого значения 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172d66d-565d-49ab-80de-d9001ffe7c4f",
   "metadata": {},
   "source": [
    "## Итоговой вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d37ad-3860-4c01-ad0e-be59756274c9",
   "metadata": {},
   "source": [
    "1. Загрузили данные и подготовили данные:\n",
    "    * убедились, что нет пропусков и дубликатов\n",
    "    * лемматизировали данные\n",
    "    * избавились от ненужных символов с помощью регулярного выражения\n",
    "    * избавились от неинформативной колонки\n",
    "2. Обучение моделей:\n",
    "    * произвели эмбеддинг признаков с помощью готовой модели BERT, обучили модели и получили следующие значения метрики *f1*: \n",
    "        * Логическая регрессия - 0.63\n",
    "        * Случайный лес - 0.24\n",
    "        * LightGBM - 0.66\n",
    "    * произвели векторизацию признаков с помощью метрики *TF-IDF*, обучили модели и получили следующие значения метрики *f1*: \n",
    "        * Логическая регрессия - 0.75\n",
    "        * Случайный лес - 0.0 \n",
    "        * LightGBM - 0.71\n",
    "3. Лучше всего себя показала  модель логической регресии с векторизованными признаками  с помощью метрики *TF-IDF* (**f1=0.75**) попутно достигнув целевого значения 0.75.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4313b2-18dd-40de-bcb4-85920816bd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
